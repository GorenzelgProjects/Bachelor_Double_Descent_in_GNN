digraph {
	graph [size="52.949999999999996,52.949999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2106233048896 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	2106232434560 [label=AddmmBackward0]
	2106232438256 -> 2106232434560
	2105392379632 [label="linear.bias
 (10)" fillcolor=lightblue]
	2105392379632 -> 2106232438256
	2106232438256 [label=AccumulateGrad]
	2106232438160 -> 2106232434560
	2106232438160 [label=ViewBackward0]
	2106232430816 -> 2106232438160
	2106232430816 [label=AvgPool2DBackward0]
	2106233036208 -> 2106232430816
	2106233036208 [label=AddBackward0]
	2106233036112 -> 2106233036208
	2106233036112 [label=ConvolutionBackward0]
	2106233030928 -> 2106233036112
	2106233030928 [label=ReluBackward0]
	2106233036448 -> 2106233030928
	2106233036448 [label=CudnnBatchNormBackward0]
	2106233038368 -> 2106233036448
	2106233038368 [label=ConvolutionBackward0]
	2106233031216 -> 2106233038368
	2106233031216 [label=ReluBackward0]
	2106233039136 -> 2106233031216
	2106233039136 [label=CudnnBatchNormBackward0]
	2106233036160 -> 2106233039136
	2106233036160 [label=AddBackward0]
	2106233039376 -> 2106233036160
	2106233039376 [label=ConvolutionBackward0]
	2106233039520 -> 2106233039376
	2106233039520 [label=ReluBackward0]
	2106233039664 -> 2106233039520
	2106233039664 [label=CudnnBatchNormBackward0]
	2106233039856 -> 2106233039664
	2106233039856 [label=ConvolutionBackward0]
	2106233033520 -> 2106233039856
	2106233033520 [label=ReluBackward0]
	2106233033376 -> 2106233033520
	2106233033376 [label=CudnnBatchNormBackward0]
	2106233033280 -> 2106233033376
	2106233033280 [label=AddBackward0]
	2106233033088 -> 2106233033280
	2106233033088 [label=ConvolutionBackward0]
	2106233032944 -> 2106233033088
	2106233032944 [label=ReluBackward0]
	2106233029248 -> 2106233032944
	2106233029248 [label=CudnnBatchNormBackward0]
	2106233029152 -> 2106233029248
	2106233029152 [label=ConvolutionBackward0]
	2106233032560 -> 2106233029152
	2106233032560 [label=ReluBackward0]
	2106233031648 -> 2106233032560
	2106233031648 [label=CudnnBatchNormBackward0]
	2106233033136 -> 2106233031648
	2106233033136 [label=AddBackward0]
	2106233031408 -> 2106233033136
	2106233031408 [label=ConvolutionBackward0]
	2106233031264 -> 2106233031408
	2106233031264 [label=ReluBackward0]
	2106233040048 -> 2106233031264
	2106233040048 [label=CudnnBatchNormBackward0]
	2106233028768 -> 2106233040048
	2106233028768 [label=ConvolutionBackward0]
	2106233038848 -> 2106233028768
	2106233038848 [label=ReluBackward0]
	2106233038704 -> 2106233038848
	2106233038704 [label=CudnnBatchNormBackward0]
	2106233038608 -> 2106233038704
	2106233038608 [label=AddBackward0]
	2106233028720 -> 2106233038608
	2106233028720 [label=ConvolutionBackward0]
	2106233039952 -> 2106233028720
	2106233039952 [label=ReluBackward0]
	2106233036016 -> 2106233039952
	2106233036016 [label=CudnnBatchNormBackward0]
	2106233035920 -> 2106233036016
	2106233035920 [label=ConvolutionBackward0]
	2106233035728 -> 2106233035920
	2106233035728 [label=ReluBackward0]
	2106233035584 -> 2106233035728
	2106233035584 [label=CudnnBatchNormBackward0]
	2106233038464 -> 2106233035584
	2106233038464 [label=AddBackward0]
	2106233035344 -> 2106233038464
	2106233035344 [label=ConvolutionBackward0]
	2106233035200 -> 2106233035344
	2106233035200 [label=ReluBackward0]
	2106233035056 -> 2106233035200
	2106233035056 [label=CudnnBatchNormBackward0]
	2106233034960 -> 2106233035056
	2106233034960 [label=ConvolutionBackward0]
	2106233034768 -> 2106233034960
	2106233034768 [label=ReluBackward0]
	2106233034624 -> 2106233034768
	2106233034624 [label=CudnnBatchNormBackward0]
	2106233034528 -> 2106233034624
	2106233034528 [label=AddBackward0]
	2106233034336 -> 2106233034528
	2106233034336 [label=ConvolutionBackward0]
	2106233034192 -> 2106233034336
	2106233034192 [label=ReluBackward0]
	2106233032224 -> 2106233034192
	2106233032224 [label=CudnnBatchNormBackward0]
	2106233032128 -> 2106233032224
	2106233032128 [label=ConvolutionBackward0]
	2106233031936 -> 2106233032128
	2106233031936 [label=ReluBackward0]
	2106233031792 -> 2106233031936
	2106233031792 [label=CudnnBatchNormBackward0]
	2106233034384 -> 2106233031792
	2106233034384 [label=AddBackward0]
	2106233030640 -> 2106233034384
	2106233030640 [label=ConvolutionBackward0]
	2106233030496 -> 2106233030640
	2106233030496 [label=ReluBackward0]
	2106233030352 -> 2106233030496
	2106233030352 [label=CudnnBatchNormBackward0]
	2106233030256 -> 2106233030352
	2106233030256 [label=ConvolutionBackward0]
	2106233030064 -> 2106233030256
	2106233030064 [label=ReluBackward0]
	2106233029056 -> 2106233030064
	2106233029056 [label=CudnnBatchNormBackward0]
	2106233030688 -> 2106233029056
	2106233030688 [label=ConvolutionBackward0]
	2106233036832 -> 2106233030688
	2105220003888 [label="conv1.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	2105220003888 -> 2106233036832
	2106233036832 [label=AccumulateGrad]
	2106233036688 -> 2106233029056
	2105219995728 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2105219995728 -> 2106233036688
	2106233036688 [label=AccumulateGrad]
	2106233036640 -> 2106233029056
	2105220003728 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2105220003728 -> 2106233036640
	2106233036640 [label=AccumulateGrad]
	2106233030112 -> 2106233030256
	2105217953088 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2105217953088 -> 2106233030112
	2106233030112 [label=AccumulateGrad]
	2106233030304 -> 2106233030352
	2105220002448 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2105220002448 -> 2106233030304
	2106233030304 [label=AccumulateGrad]
	2106233030448 -> 2106233030352
	2105217952848 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2105217952848 -> 2106233030448
	2106233030448 [label=AccumulateGrad]
	2106233030544 -> 2106233030640
	2105217952928 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2105217952928 -> 2106233030544
	2106233030544 [label=AccumulateGrad]
	2106233030688 -> 2106233034384
	2106233031696 -> 2106233031792
	2105217947968 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2105217947968 -> 2106233031696
	2106233031696 [label=AccumulateGrad]
	2106233031744 -> 2106233031792
	2105368683008 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2105368683008 -> 2106233031744
	2106233031744 [label=AccumulateGrad]
	2106233031984 -> 2106233032128
	2105358950752 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2105358950752 -> 2106233031984
	2106233031984 [label=AccumulateGrad]
	2106233032176 -> 2106233032224
	2105358945552 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2105358945552 -> 2106233032176
	2106233032176 [label=AccumulateGrad]
	2106233032320 -> 2106233032224
	2105358946832 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2105358946832 -> 2106233032320
	2106233032320 [label=AccumulateGrad]
	2106233034240 -> 2106233034336
	2105358957952 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2105358957952 -> 2106233034240
	2106233034240 [label=AccumulateGrad]
	2106233034384 -> 2106233034528
	2106233034576 -> 2106233034624
	2105220271136 [label="layer2.0.bn1.weight
 (64)" fillcolor=lightblue]
	2105220271136 -> 2106233034576
	2106233034576 [label=AccumulateGrad]
	2106233034720 -> 2106233034624
	2105358958032 [label="layer2.0.bn1.bias
 (64)" fillcolor=lightblue]
	2105358958032 -> 2106233034720
	2106233034720 [label=AccumulateGrad]
	2106233034816 -> 2106233034960
	2105358949152 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2105358949152 -> 2106233034816
	2106233034816 [label=AccumulateGrad]
	2106233035008 -> 2106233035056
	2105358953712 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2105358953712 -> 2106233035008
	2106233035008 [label=AccumulateGrad]
	2106233035152 -> 2106233035056
	2105358947232 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2105358947232 -> 2106233035152
	2106233035152 [label=AccumulateGrad]
	2106233035248 -> 2106233035344
	2105358948192 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2105358948192 -> 2106233035248
	2106233035248 [label=AccumulateGrad]
	2106233035392 -> 2106233038464
	2106233035392 [label=ConvolutionBackward0]
	2106233034768 -> 2106233035392
	2106233034912 -> 2106233035392
	2105358955232 [label="layer2.0.shortcut.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2105358955232 -> 2106233034912
	2106233034912 [label=AccumulateGrad]
	2106233035488 -> 2106233035584
	2105358955312 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2105358955312 -> 2106233035488
	2106233035488 [label=AccumulateGrad]
	2106233035536 -> 2106233035584
	2105358948512 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2105358948512 -> 2106233035536
	2106233035536 [label=AccumulateGrad]
	2106233035776 -> 2106233035920
	2105358942272 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2105358942272 -> 2106233035776
	2106233035776 [label=AccumulateGrad]
	2106233035968 -> 2106233036016
	2105358955792 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2105358955792 -> 2106233035968
	2106233035968 [label=AccumulateGrad]
	2106233040000 -> 2106233036016
	2105358942352 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2105358942352 -> 2106233040000
	2106233040000 [label=AccumulateGrad]
	2106233039904 -> 2106233028720
	2105358955952 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2105358955952 -> 2106233039904
	2106233039904 [label=AccumulateGrad]
	2106233038464 -> 2106233038608
	2106233038656 -> 2106233038704
	2105358956032 [label="layer3.0.bn1.weight
 (128)" fillcolor=lightblue]
	2105358956032 -> 2106233038656
	2106233038656 [label=AccumulateGrad]
	2106233038800 -> 2106233038704
	2105358942512 [label="layer3.0.bn1.bias
 (128)" fillcolor=lightblue]
	2105358942512 -> 2106233038800
	2106233038800 [label=AccumulateGrad]
	2106233030976 -> 2106233028768
	2105358949472 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2105358949472 -> 2106233030976
	2106233030976 [label=AccumulateGrad]
	2106233033664 -> 2106233040048
	2105358956512 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2105358956512 -> 2106233033664
	2106233033664 [label=AccumulateGrad]
	2106233040144 -> 2106233040048
	2105358943072 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2105358943072 -> 2106233040144
	2106233040144 [label=AccumulateGrad]
	2106233031312 -> 2106233031408
	2105358949792 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2105358949792 -> 2106233031312
	2106233031312 [label=AccumulateGrad]
	2106233031456 -> 2106233033136
	2106233031456 [label=ConvolutionBackward0]
	2106233038848 -> 2106233031456
	2106233031072 -> 2106233031456
	2105358956752 [label="layer3.0.shortcut.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2105358956752 -> 2106233031072
	2106233031072 [label=AccumulateGrad]
	2106233031552 -> 2106233031648
	2105392375392 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2105392375392 -> 2106233031552
	2106233031552 [label=AccumulateGrad]
	2106233031600 -> 2106233031648
	2105392372672 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2105392372672 -> 2106233031600
	2106233031600 [label=AccumulateGrad]
	2106233028816 -> 2106233029152
	2105392371632 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2105392371632 -> 2106233028816
	2106233028816 [label=AccumulateGrad]
	2106233029200 -> 2106233029248
	2105392371872 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2105392371872 -> 2106233029200
	2106233029200 [label=AccumulateGrad]
	2106233032896 -> 2106233029248
	2105392371792 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2105392371792 -> 2106233032896
	2106233032896 [label=AccumulateGrad]
	2106233032992 -> 2106233033088
	2105392376672 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2105392376672 -> 2106233032992
	2106233032992 [label=AccumulateGrad]
	2106233033136 -> 2106233033280
	2106233033328 -> 2106233033376
	2105392376752 [label="layer4.0.bn1.weight
 (256)" fillcolor=lightblue]
	2105392376752 -> 2106233033328
	2106233033328 [label=AccumulateGrad]
	2106233033472 -> 2106233033376
	2105392376912 [label="layer4.0.bn1.bias
 (256)" fillcolor=lightblue]
	2105392376912 -> 2106233033472
	2106233033472 [label=AccumulateGrad]
	2106233033568 -> 2106233039856
	2105392377392 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2105392377392 -> 2106233033568
	2106233033568 [label=AccumulateGrad]
	2106233039712 -> 2106233039664
	2105392377472 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2105392377472 -> 2106233039712
	2106233039712 [label=AccumulateGrad]
	2106233039568 -> 2106233039664
	2105392377552 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2105392377552 -> 2106233039568
	2106233039568 [label=AccumulateGrad]
	2106233039472 -> 2106233039376
	2105392378112 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2105392378112 -> 2106233039472
	2106233039472 [label=AccumulateGrad]
	2106233039328 -> 2106233036160
	2106233039328 [label=ConvolutionBackward0]
	2106233033520 -> 2106233039328
	2106233039808 -> 2106233039328
	2105392378272 [label="layer4.0.shortcut.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2105392378272 -> 2106233039808
	2106233039808 [label=AccumulateGrad]
	2106233039232 -> 2106233039136
	2105392378352 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2105392378352 -> 2106233039232
	2106233039232 [label=AccumulateGrad]
	2106233039184 -> 2106233039136
	2105392378432 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2105392378432 -> 2106233039184
	2106233039184 [label=AccumulateGrad]
	2106233031168 -> 2106233038368
	2105392378912 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2105392378912 -> 2106233031168
	2106233031168 [label=AccumulateGrad]
	2106233038320 -> 2106233036448
	2105392378992 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2105392378992 -> 2106233038320
	2106233038320 [label=AccumulateGrad]
	2106233030880 -> 2106233036448
	2105392379072 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2105392379072 -> 2106233030880
	2106233030880 [label=AccumulateGrad]
	2106233031024 -> 2106233036112
	2105392379552 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2105392379552 -> 2106233031024
	2106233031024 [label=AccumulateGrad]
	2106233036160 -> 2106233036208
	2106232431200 -> 2106232434560
	2106232431200 [label=TBackward0]
	2106232429040 -> 2106232431200
	2105392379472 [label="linear.weight
 (10, 512)" fillcolor=lightblue]
	2105392379472 -> 2106232429040
	2106232429040 [label=AccumulateGrad]
	2106232434560 -> 2106233048896
}
